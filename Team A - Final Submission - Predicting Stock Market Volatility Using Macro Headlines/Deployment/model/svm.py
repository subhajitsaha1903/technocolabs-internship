# -*- coding: utf-8 -*-
"""_SVM_sameday_sentiment_RBF_final_stockdata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hquNdn13N64o5-cSQtnUdIoO0ZUFdBpg
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

data = pd.read_csv('labelled_dataset_full.csv')
#data_1=data.drop('lab_prevday')
data_1=data[['lab_sameday','Polarity','Open','High','Volume','Low']]
data_1.dropna(inplace=True)
data_1



import seaborn as sns

# calculate the correlations
correlations = data_1.corr()

# plot the heatmap 
sns.heatmap(correlations, xticklabels=correlations.columns, yticklabels=correlations.columns, annot=True)

# plot the clustermap 
sns.clustermap(correlations, xticklabels=correlations.columns, yticklabels=correlations.columns, annot=True)
#remove NUM_NEU and TOT_WORDS which are highly correlated

# Graph to show the Higly Imbalanced Data. To Predict the Column ['lab_sameday']. Right now.. majority of records/rows with 'Target=0'
# And few records with 'Target = 1'
pd.value_counts(data_1['lab_sameday']).plot.bar()
plt.title('lab_sameday class histogram')
plt.xlabel('lab_sameday')

# Class count
count_class_1, count_class_0 = data_1.lab_sameday.value_counts()
# Divide by class
df_class_0 = data_1[data_1['lab_sameday'] == 0]
df_class_1 = data_1[data_1['lab_sameday'] == 1]
print('Zero',count_class_0)
print('One',count_class_1)

# Doing Random Undersampling to balance data
df_class_1_under = df_class_1.sample(count_class_0)
df_test_under = pd.concat([df_class_1_under, df_class_0], axis=0)
print('After Random under-sampling:')
print(df_test_under.lab_sameday.value_counts())
df_test_under.lab_sameday.value_counts().plot(kind='bar', title='Count(lab_sameday)');
data_2=df_test_under
data_2

X=data_2[['Polarity','High','Low','Open']]
y=data_2['lab_sameday'].values
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=0)
# shape of the dataset
print('Shape of training data :',X_train.shape)
print('Shape of testing data :',X_test.shape)

# draw boxplots to visualize outliers
plt.figure(figsize=(12,5))
plt.subplot(1, 2, 1)
fig = data_2.boxplot(column='Open')
fig.set_title('outliers')
fig.set_ylabel('values')


plt.subplot(1, 2, 2)
fig = data_2.boxplot(column='Polarity')
fig.set_title('outliers')
fig.set_ylabel('Polarity')
#The below boxplots confirm that there are lot of outliers in these variables.
#since the dataset contains outliers, so the value of C should be high while training the model.

# plot histogram to check distribution,to find out if they are normal or skewed.

plt.figure(figsize=(12,5))
plt.subplot(1, 2, 1)
fig = data_2['Open'].hist(bins=20)
fig.set_xlabel('Open')
fig.set_ylabel('Number of words')


plt.subplot(1, 2, 2)
fig = data_2['Polarity'].hist(bins=20)
fig.set_xlabel('Polarity')
fig.set_ylabel('Number of words')
#All distributions are skewed

# Standard Scalar
#from sklearn.preprocessing import StandardScaler
#sc = StandardScaler()
#X_train = sc.fit_transform(X_train)
#X_test = sc.transform(X_test)
#Normalisation
from sklearn import preprocessing
# normalize the data attributes
X_train= preprocessing.normalize(X_train)
X_test= preprocessing.normalize(X_test)
import sklearn
from sklearn import svm
from collections import Counter

#SVM RBF kernel
clf = svm.SVC(C=1000,kernel='rbf',random_state=0)
clf.fit(X_train, y_train)
confidence = clf.score(X_test, y_test)
print('accuracy:',confidence)
y_pred_test = clf.predict(X_test)
print('predicted class counts:',Counter(y_pred_test))
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred_test))
print(classification_report(y_test, y_pred_test))

"""Saving the model"""

import pickle
pickle.dump(clf,open('Pickle_svm_sameday_stock3.pkl','wb'))

from sklearn.metrics import accuracy_score
# instantiate classifier with default hyperparam
#Compare the train-set and test-set accuracy
#compare the train-set and test-set accuracy to check for overfitting
y_pred_train = clf.predict(X_train)
#y_pred_train
print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))
# print the scores on training and test set
print('Training set score: {:.4f}'.format(clf.score(X_train, y_train)))
print('Test set score: {:.4f}'.format(clf.score(X_test, y_test)))
#Compare model accuracy with null accuracy.Null accuracy is the accuracy that could be achieved by always predicting the most frequent class.

# The problem is that accuracy is an inadequate measure for quantifying predictive performance in the imbalanced dataset problem.
#One such metric to analyze the model performance in imbalanced classes problem is Confusion matrix.
# Print the Confusion Matrix and slice it into four pieces
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred_test)
print('Confusion matrix\n\n', cm)
print('\nTrue Positives(TP) = ', cm[0,0])
print('\nTrue Negatives(TN) = ', cm[1,1])
print('\nFalse Positives(FP) = ', cm[0,1])
print('\nFalse Negatives(FN) = ', cm[1,0])

# visualize confusion matrix with seaborn heatmap
cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], 
                                 index=['Predict Positive:1', 'Predict Negative:0'])

sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')

#Classification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model. 
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_test))

TP = cm[0,0]
TN = cm[1,1]
FP = cm[0,1]
FN = cm[1,0]
# print classification accuracy
classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)
print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))

# print classification error
classification_error = (FP + FN) / float(TP + TN + FP + FN)
print('Classification error : {0:0.4f}'.format(classification_error))
#Precision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes.
# print precision score
precision = TP / float(TP + FP)
print('Precision : {0:0.4f}'.format(precision))
#Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes.
recall = TP / float(TP + FN)
print('Recall or Sensitivity : {0:0.4f}'.format(recall))
#True Positive Rate is synonymous with Recall.
true_positive_rate = TP / float(TP + FN)
print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))
#False Positive Rate
false_positive_rate = FP / float(FP + TN)
print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))
#specificity
specificity = TN / (TN + FP)
print('Specificity : {0:0.4f}'.format(specificity))
#f1-score is the weighted harmonic mean of precision and recall. The best possible f1-score would be 1.0 and the worst would be 0.0.
#Support is the actual number of occurrences of the class in our dataset.

#ROC - AUC 
#ROC Curve stands for Receiver Operating Characteristic Curve. An ROC Curve is a plot which shows the performance of a classification model at various classification threshold levels.
#The ROC Curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold levels.
# plot ROC Curve
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_test)
plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, linewidth=2)
plt.plot([0,1], [0,1], 'k--' )
plt.rcParams['font.size'] = 12
plt.title('ROC curve')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.show()
#ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.
#ROC AUC-ROC AUC stands for Receiver Operating Characteristic - Area Under Curve
#A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.
# compute ROC AUC
from sklearn.metrics import roc_auc_score
ROC_AUC = roc_auc_score(y_test, y_pred_test)
print('ROC AUC : {:.4f}'.format(ROC_AUC))
#ROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.
# calculate cross-validated ROC AUC 
from sklearn.model_selection import cross_val_score
Cross_validated_ROC_AUC = cross_val_score(clf, X_train, y_train, cv=10, scoring='roc_auc').mean()
print('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))